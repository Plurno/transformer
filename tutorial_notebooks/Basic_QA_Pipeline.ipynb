{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Finder\n",
    "from haystack.preprocessor.cleaning import clean_wiki_text\n",
    "from haystack.preprocessor.utils import convert_files_to_dicts, fetch_archive_from_http\n",
    "from haystack.reader.farm import FARMReader\n",
    "from haystack.reader.transformers import TransformersReader\n",
    "from haystack.utils import print_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/09/2020 14:27:04 - INFO - elasticsearch -   HEAD http://localhost:9200/ahrq [status:200 request:0.008s]\n",
      "11/09/2020 14:27:04 - INFO - elasticsearch -   HEAD http://localhost:9200/label [status:200 request:0.001s]\n"
     ]
    }
   ],
   "source": [
    "# Connect to a locally running instance of Elasticsearch\n",
    "\n",
    "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
    "# document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"ahrq\", search_fields='body')\n",
    "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"ahrq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of documents\n",
    "\n",
    "Haystack provides a customizable pipeline for:\n",
    " - converting files into texts\n",
    " - cleaning texts\n",
    " - splitting texts\n",
    " - writing them to a Document Store\n",
    "\n",
    "In this tutorial, we download Wikipedia articles about Game of Thrones, apply a basic cleaning function, and index them in Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url_text(text: str) -> str:\n",
    "    # get rid of multiple new lines\n",
    "    while \"\\n\\n\" in text:\n",
    "        text = text.replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "    # remove extremely short lines, combine small paragraphs into larger ones\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned = []\n",
    "    multi_lines = ''\n",
    "    for l in lines:\n",
    "        if len(l) > 100:\n",
    "            multi_lines += l + '\\n\\t'\n",
    "        if len(l) > 500:\n",
    "            cleaned.append(multi_lines)\n",
    "            multi_lines = ''\n",
    "    \n",
    "    if multi_lines: cleaned.append(multi_lines) \n",
    "    text = \"\\n\\n\".join(cleaned)\n",
    "\n",
    "    # add paragraphs (recognized by double new line)\n",
    "    # text = text.replace(\"\\n\", \"\\n\\n\")\n",
    "\n",
    "    # remove empty paragrahps\n",
    "    # text = re.sub(r\"(==.*==\\n\\n\\n)\", \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/05/2020 10:20:56 - INFO - elasticsearch -   POST http://localhost:9200/ahrq_annotated/_search?scroll=2s [status:200 request:0.023s]\n",
      "11/05/2020 10:20:56 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.010s]\n",
      "11/05/2020 10:20:56 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.011s]\n",
      "11/05/2020 10:20:56 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.009s]\n",
      "11/05/2020 10:20:56 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.009s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.011s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.012s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.009s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.004s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.010s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.005s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.005s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.005s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.009s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.009s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.012s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.010s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.011s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.005s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.013s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.010s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.005s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.001s]\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "# Loop through ES and save all docs as txt files so we can use convert_files_to_dict funciton\n",
    "\n",
    "host = 'localhost'\n",
    "port = 9200\n",
    "client = Elasticsearch(f'{host}:{port}')\n",
    "index = 'ahrq_annotated'\n",
    "\n",
    "match_all = {\n",
    "    \"size\": 100,\n",
    "    \"query\": {\n",
    "        \"match_all\": {}\n",
    "    }\n",
    "}\n",
    "\n",
    "resp = client.search(\n",
    "    index = index,\n",
    "    body = match_all,\n",
    "    scroll = '2s' # length of time to keep search context\n",
    ")\n",
    "\n",
    "old_scroll_id = resp['_scroll_id']\n",
    "\n",
    "dicts = []\n",
    "while len(resp['hits']['hits']):\n",
    "    resp = client.scroll(\n",
    "        scroll_id = old_scroll_id,\n",
    "        scroll = '2s' # length of time to keep search context\n",
    "    )\n",
    "\n",
    "    # check if there's a new scroll ID\n",
    "    if old_scroll_id != resp['_scroll_id']:\n",
    "        print (\"NEW SCROLL ID:\", resp['_scroll_id'])\n",
    "\n",
    "    # keep track of pass scroll _id\n",
    "    old_scroll_id = resp['_scroll_id']\n",
    "    \n",
    "    for doc in resp['hits']['hits']:\n",
    "        del doc['_source']['tags']\n",
    "        dicts.append(doc['_source'])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# Save docs as txt files\n",
    "doc_dir = \"data/ahrq_txt_files/\"\n",
    "import os\n",
    "if not os.path.exists(doc_dir):\n",
    "    os.makedirs(doc_dir)\n",
    "    \n",
    "    for d in dicts:\n",
    "        with open(doc_dir + d['url'][8:].replace('/', '_') + '.txt', 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(d['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'In 2016, AHRQ launched a program of grants and contracts aimed at helping health care providers move patient-centered outcomes research (PCOR) evidence into practice through clinical decision support (CDS). AHRQ advances the science of CDS by supporting implementers, clinicians, and technology vendors in developing CDS tools that are shareable, standards-based, publicly-available, and patient-centered. The four components are detailed below. \\n\\tAHRQ awarded RTI International a cooperative agreement to become the Patient-Centered Clinical Decision Support Learning Network (PCCDS-LN). The PCCDS-LN built a community of researchers, clinicians, professional societies, and others that explored and advanced patient-centered CDS. While the grant ended in 2020, many of the Learning Network’s resources continue to be available. \\n\\tAHRQ awarded the MITRE Corporation a contract to develop \"CDS Connect,\" an online platform that includes a repository of CDS artifacts, an authoring tool to enable creation of CDS, and prototype tools for sharing and testing CDS. MITRE also hosts the monthly CDS Connect Work Group meeting. \\n\\t', 'meta': {'name': 'cds.ahrq.gov.txt'}}, {'text': 'p.boxlink > a[href$=\".pdf\"]::before, p.boxlink > a[href$=\".docx\"]::before, p.boxlink > a.document::before {\\n\\t', 'meta': {'name': 'www.ahrq.gov_antibiotic-use_acute-care_diagnosis_bacteremia.html.txt'}}, {'text': 'p.boxlink > a[href$=\".pdf\"]::before, p.boxlink > a[href$=\".docx\"]::before, p.boxlink > a.document::before {\\n\\t', 'meta': {'name': 'www.ahrq.gov_antibiotic-use_acute-care_diagnosis_cdi.html.txt'}}]\n"
     ]
    }
   ],
   "source": [
    "dicts = convert_files_to_dicts(dir_path=doc_dir, clean_func=clean_url_text, split_paragraphs=True)\n",
    "print(dicts[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1559.146375123992"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sum([len(d['text']) for d in dicts])\n",
    "s/len(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/05/2020 10:22:03 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:0.889s]\n",
      "11/05/2020 10:22:04 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.144s]\n",
      "11/05/2020 10:22:05 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.162s]\n",
      "11/05/2020 10:22:06 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.079s]\n",
      "11/05/2020 10:22:07 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.118s]\n",
      "11/05/2020 10:22:09 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.128s]\n",
      "11/05/2020 10:22:10 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.082s]\n",
      "11/05/2020 10:22:11 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.154s]\n",
      "11/05/2020 10:22:12 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.159s]\n",
      "11/05/2020 10:22:13 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.080s]\n",
      "11/05/2020 10:22:15 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.150s]\n",
      "11/05/2020 10:22:16 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.508s]\n",
      "11/05/2020 10:22:18 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.221s]\n",
      "11/05/2020 10:22:19 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.130s]\n",
      "11/05/2020 10:22:20 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.105s]\n",
      "11/05/2020 10:22:21 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.137s]\n",
      "11/05/2020 10:22:22 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.069s]\n",
      "11/05/2020 10:22:24 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.200s]\n",
      "11/05/2020 10:22:25 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.249s]\n",
      "11/05/2020 10:22:26 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.254s]\n",
      "11/05/2020 10:22:29 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:2.462s]\n",
      "11/05/2020 10:22:30 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.096s]\n",
      "11/05/2020 10:22:31 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.077s]\n",
      "11/05/2020 10:22:32 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.083s]\n",
      "11/05/2020 10:22:33 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.001s]\n",
      "11/05/2020 10:22:35 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.166s]\n",
      "11/05/2020 10:22:36 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.128s]\n",
      "11/05/2020 10:22:37 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.083s]\n",
      "11/05/2020 10:22:38 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.108s]\n",
      "11/05/2020 10:22:39 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.094s]\n",
      "11/05/2020 10:22:41 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.475s]\n",
      "11/05/2020 10:22:42 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.132s]\n",
      "11/05/2020 10:22:43 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.092s]\n",
      "11/05/2020 10:22:45 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.065s]\n",
      "11/05/2020 10:22:46 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.143s]\n",
      "11/05/2020 10:22:47 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.170s]\n",
      "11/05/2020 10:22:48 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.130s]\n",
      "11/05/2020 10:22:49 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.091s]\n",
      "11/05/2020 10:22:51 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.174s]\n",
      "11/05/2020 10:22:52 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.160s]\n",
      "11/05/2020 10:22:53 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.176s]\n",
      "11/05/2020 10:22:54 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.083s]\n",
      "11/05/2020 10:22:56 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.097s]\n",
      "11/05/2020 10:22:57 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.086s]\n",
      "11/05/2020 10:22:58 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.094s]\n",
      "11/05/2020 10:22:59 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.358s]\n",
      "11/05/2020 10:23:00 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.083s]\n"
     ]
    }
   ],
   "source": [
    "# Now, let's write the dicts containing documents to our DB.\n",
    "document_store.write_documents(dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initalize Retriever, Reader,  & Finder\n",
    "\n",
    "### Retriever\n",
    "\n",
    "Retrievers help narrowing down the scope for the Reader to smaller units of text where a given question could be answered.\n",
    "They use some simple but fast algorithm.\n",
    "\n",
    "**Here:** We use Elasticsearch's default BM25 algorithm\n",
    "\n",
    "**Alternatives:**\n",
    "\n",
    "- Customize the `ElasticsearchRetriever`with custom queries (e.g. boosting) and filters\n",
    "- Use `TfidfRetriever` in combination with a SQL or InMemory Document store for simple prototyping and debugging\n",
    "- Use `EmbeddingRetriever` to find candidate documents based on the similarity of embeddings (e.g. created via Sentence-BERT)\n",
    "- Use `DensePassageRetriever` to use different embedding models for passage and query (see Tutorial 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.retriever.sparse import ElasticsearchRetriever\n",
    "retriever = ElasticsearchRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Alternative: An in-memory TfidfRetriever based on Pandas dataframes for building quick-prototypes with SQLite document store.\n",
    "\n",
    "# from haystack.retriever.sparse import TfidfRetriever\n",
    "# retriever = TfidfRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reader\n",
    "\n",
    "A Reader scans the texts returned by retrievers in detail and extracts the k best answers. They are based\n",
    "on powerful, but slower deep learning models.\n",
    "\n",
    "Haystack currently supports Readers based on the frameworks FARM and Transformers.\n",
    "With both you can either load a local model or one from Hugging Face's model hub (https://huggingface.co/models).\n",
    "\n",
    "**Here:** a medium sized RoBERTa QA model using a Reader based on FARM (https://huggingface.co/deepset/roberta-base-squad2)\n",
    "\n",
    "**Alternatives (Reader):** TransformersReader (leveraging the `pipeline` of the Transformers package)\n",
    "\n",
    "**Alternatives (Models):** e.g. \"distilbert-base-uncased-distilled-squad\" (fast) or \"deepset/bert-large-uncased-whole-word-masking-squad2\" (good accuracy)\n",
    "\n",
    "**Hint:** You can adjust the model to return \"no answer possible\" with the no_ans_boost. Higher values mean the model prefers \"no answer possible\"\n",
    "\n",
    "#### FARMReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/09/2020 14:36:26 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n",
      "11/09/2020 14:36:26 - INFO - farm.infer -   Could not find `deepset/roberta-base-squad2` locally. Try to download from model hub ...\n",
      "11/09/2020 14:36:29 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "11/09/2020 14:36:34 - WARNING - farm.modeling.prediction_head -   Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"loss_ignore_index\": -1}\n",
      "11/09/2020 14:36:35 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n",
      "11/09/2020 14:36:35 - INFO - farm.infer -   Got ya 7 parallel workers to do inference ...\n",
      "11/09/2020 14:36:35 - INFO - farm.infer -    0    0    0    0    0    0    0 \n",
      "11/09/2020 14:36:35 - INFO - farm.infer -   /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\\n",
      "11/09/2020 14:36:35 - INFO - farm.infer -   /'\\  / \\  /'\\  /'\\  / \\  / \\  /'\\\n",
      "11/09/2020 14:36:35 - INFO - farm.infer -               \n"
     ]
    }
   ],
   "source": [
    "# Load a  local model or any of the QA models on\n",
    "# Hugging Face's model hub (https://huggingface.co/models)\n",
    "\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finder\n",
    "\n",
    "The Finder sticks together reader and retriever in a pipeline to answer our actual questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "finder = Finder(reader, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voilà! Ask a question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/09/2020 14:37:46 - INFO - elasticsearch -   POST http://localhost:9200/ahrq/_search [status:200 request:0.781s]\n",
      "11/09/2020 14:37:46 - INFO - haystack.retriever.sparse -   Got 10 candidates from retriever\n",
      "11/09/2020 14:37:46 - INFO - haystack.finder -   Reader is looking for detailed answer in 12711 chars ...\n"
     ]
    }
   ],
   "source": [
    "# You can configure how many candidates the reader and retriever shall return\n",
    "# The higher top_k_retriever, the better (but also the slower) your answers. \n",
    "prediction = finder.get_answers(question=\"Does ahrq offer minority supplements for grants?\", top_k_retriever=10, top_k_reader=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_gpu_memory():\n",
    "  _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "\n",
    "  ACCEPTABLE_AVAILABLE_MEMORY = 1024\n",
    "  COMMAND = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "  memory_free_info = _output_to_list(sp.check_output(COMMAND.split()))[1:]\n",
    "  memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "  print(memory_free_values)\n",
    "  return memory_free_values\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
