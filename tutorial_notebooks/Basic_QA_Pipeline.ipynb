{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Finder\n",
    "from haystack.preprocessor.cleaning import clean_wiki_text\n",
    "from haystack.preprocessor.utils import convert_files_to_dicts, fetch_archive_from_http\n",
    "from haystack.reader.farm import FARMReader\n",
    "from haystack.reader.transformers import TransformersReader\n",
    "from haystack.utils import print_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/10/2020 17:02:14 - INFO - elasticsearch -   HEAD http://localhost:9200/ahrq [status:200 request:0.053s]\n",
      "11/10/2020 17:02:14 - INFO - elasticsearch -   HEAD http://localhost:9200/label [status:200 request:0.016s]\n"
     ]
    }
   ],
   "source": [
    "# Connect to a locally running instance of Elasticsearch\n",
    "\n",
    "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
    "# document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"ahrq\", search_fields='body')\n",
    "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"ahrq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of documents\n",
    "\n",
    "Haystack provides a customizable pipeline for:\n",
    " - converting files into texts\n",
    " - cleaning texts\n",
    " - splitting texts\n",
    " - writing them to a Document Store\n",
    "\n",
    "In this tutorial, we download Wikipedia articles about Game of Thrones, apply a basic cleaning function, and index them in Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/05/2020 10:20:56 - INFO - elasticsearch -   POST http://localhost:9200/ahrq_annotated/_search?scroll=2s [status:200 request:0.023s]\n",
      "11/05/2020 10:20:56 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.010s]\n",
      "11/05/2020 10:20:56 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.011s]\n",
      "11/05/2020 10:20:56 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.009s]\n",
      "11/05/2020 10:20:56 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.009s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.011s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.012s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.009s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.004s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.010s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.005s]\n",
      "11/05/2020 10:20:57 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.005s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.005s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.009s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.009s]\n",
      "11/05/2020 10:20:58 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.012s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.010s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.011s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.005s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.013s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:20:59 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.010s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.008s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.007s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.006s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.005s]\n",
      "11/05/2020 10:21:00 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll?scroll=2s [status:200 request:0.001s]\n"
     ]
    }
   ],
   "source": [
    "def clean_url_text(text: str) -> str:\n",
    "    # get rid of multiple new lines\n",
    "    while \"\\n\\n\" in text:\n",
    "        text = text.replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "    # remove extremely short lines, combine small paragraphs into larger ones\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned = []\n",
    "    multi_lines = ''\n",
    "    for l in lines:\n",
    "        if len(l) > 100:\n",
    "            multi_lines += l + '\\n\\t'\n",
    "        if len(l) > 500:\n",
    "            cleaned.append(multi_lines)\n",
    "            multi_lines = ''\n",
    "    \n",
    "    if multi_lines: cleaned.append(multi_lines) \n",
    "    text = \"\\n\\n\".join(cleaned)\n",
    "\n",
    "    # add paragraphs (recognized by double new line)\n",
    "    # text = text.replace(\"\\n\", \"\\n\\n\")\n",
    "\n",
    "    # remove empty paragrahps\n",
    "    # text = re.sub(r\"(==.*==\\n\\n\\n)\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "# Loop through ES and save all docs as txt files so we can use convert_files_to_dict funciton\n",
    "\n",
    "host = 'localhost'\n",
    "port = 9200\n",
    "client = Elasticsearch(f'{host}:{port}')\n",
    "index = 'ahrq_annotated'\n",
    "\n",
    "match_all = {\n",
    "    \"size\": 100,\n",
    "    \"query\": {\n",
    "        \"match_all\": {}\n",
    "    }\n",
    "}\n",
    "\n",
    "resp = client.search(\n",
    "    index = index,\n",
    "    body = match_all,\n",
    "    scroll = '2s' # length of time to keep search context\n",
    ")\n",
    "\n",
    "old_scroll_id = resp['_scroll_id']\n",
    "\n",
    "dicts = []\n",
    "while len(resp['hits']['hits']):\n",
    "    resp = client.scroll(\n",
    "        scroll_id = old_scroll_id,\n",
    "        scroll = '2s' # length of time to keep search context\n",
    "    )\n",
    "\n",
    "    # check if there's a new scroll ID\n",
    "    if old_scroll_id != resp['_scroll_id']:\n",
    "        print (\"NEW SCROLL ID:\", resp['_scroll_id'])\n",
    "\n",
    "    # keep track of pass scroll _id\n",
    "    old_scroll_id = resp['_scroll_id']\n",
    "    \n",
    "    for doc in resp['hits']['hits']:\n",
    "        del doc['_source']['tags']\n",
    "        dicts.append(doc['_source'])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# Save docs as txt files\n",
    "doc_dir = \"data/ahrq_txt_files/\"\n",
    "import os\n",
    "if not os.path.exists(doc_dir):\n",
    "    os.makedirs(doc_dir)\n",
    "    \n",
    "    for d in dicts:\n",
    "        with open(doc_dir + d['url'][8:].replace('/', '_') + '.txt', 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(d['body'])\n",
    "            \n",
    "dicts = convert_files_to_dicts(dir_path=doc_dir, clean_func=clean_url_text, split_paragraphs=True)\n",
    "print(dicts[:3])\n",
    "\n",
    "s = sum([len(d['text']) for d in dicts])\n",
    "print('average lenght of paragraph: ', s/len(dicts)\n",
    "\n",
    "# Now, let's write the dicts containing documents to our DB.\n",
    "document_store.write_documents(dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initalize Retriever, Reader,  & Finder\n",
    "\n",
    "### Retriever\n",
    "\n",
    "Retrievers help narrowing down the scope for the Reader to smaller units of text where a given question could be answered.\n",
    "They use some simple but fast algorithm.\n",
    "\n",
    "**Here:** We use Elasticsearch's default BM25 algorithm\n",
    "\n",
    "**Alternatives:**\n",
    "\n",
    "- Customize the `ElasticsearchRetriever`with custom queries (e.g. boosting) and filters\n",
    "- Use `TfidfRetriever` in combination with a SQL or InMemory Document store for simple prototyping and debugging\n",
    "- Use `EmbeddingRetriever` to find candidate documents based on the similarity of embeddings (e.g. created via Sentence-BERT)\n",
    "- Use `DensePassageRetriever` to use different embedding models for passage and query (see Tutorial 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reader\n",
    "\n",
    "A Reader scans the texts returned by retrievers in detail and extracts the k best answers. They are based\n",
    "on powerful, but slower deep learning models.\n",
    "\n",
    "Haystack currently supports Readers based on the frameworks FARM and Transformers.\n",
    "With both you can either load a local model or one from Hugging Face's model hub (https://huggingface.co/models).\n",
    "\n",
    "**Here:** a medium sized RoBERTa QA model using a Reader based on FARM (https://huggingface.co/deepset/roberta-base-squad2)\n",
    "\n",
    "**Alternatives (Reader):** TransformersReader (leveraging the `pipeline` of the Transformers package)\n",
    "\n",
    "**Alternatives (Models):** e.g. \"distilbert-base-uncased-distilled-squad\" (fast) or \"deepset/bert-large-uncased-whole-word-masking-squad2\" (good accuracy)\n",
    "\n",
    "**Hint:** You can adjust the model to return \"no answer possible\" with the no_ans_boost. Higher values mean the model prefers \"no answer possible\"\n",
    "\n",
    "#### FARMReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finder\n",
    "\n",
    "The Finder sticks together reader and retriever in a pipeline to answer our actual questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from haystack.retriever.sparse import ElasticsearchRetriever\n",
    "retriever = ElasticsearchRetriever(document_store=document_store)\n",
    "\n",
    "# Alternative: An in-memory TfidfRetriever based on Pandas dataframes for building quick-prototypes with SQLite document store.\n",
    "\n",
    "# from haystack.retriever.sparse import TfidfRetriever\n",
    "# retriever = TfidfRetriever(document_store=document_store)\n",
    "\n",
    "\n",
    "# Load a  local model or any of the QA models on\n",
    "# Hugging Face's model hub (https://huggingface.co/models)\n",
    "\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n",
    "\n",
    "finder = Finder(reader, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voilà! Ask a question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/10/2020 17:37:00 - INFO - elasticsearch -   POST http://localhost:9200/ahrq/_search [status:200 request:0.014s]\n",
      "11/10/2020 17:37:00 - INFO - haystack.retriever.sparse -   Got 10 candidates from retriever\n",
      "11/10/2020 17:37:00 - INFO - haystack.finder -   Reader is looking for detailed answer in 11648 chars ...\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.00 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.17 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 43.49 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 41.67 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 43.49 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 45.46 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.17 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 31.26 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 45.40 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 40.05 Batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   {   'answer': 'Health and Human Services',\n",
      "        'context': 'nd Quality (AHRQ). As part of the budget for the '\n",
      "                   'Department of Health and Human Services (HHS), the '\n",
      "                   'performance budget submission both provides backgr',\n",
      "        'score': 14.955009460449219},\n",
      "    {   'answer': 'Health and Human Services',\n",
      "        'context': 'nd Quality (AHRQ). As part of the budget for the '\n",
      "                   'Department of Health and Human Services (HHS), the '\n",
      "                   'performance budget submission both provides backgr',\n",
      "        'score': 14.698270797729492},\n",
      "    {   'answer': 'Health and Human Services',\n",
      "        'context': 'nd Quality (AHRQ). As part of the budget for the '\n",
      "                   'Department of Health and Human Services (HHS), the '\n",
      "                   'performance budget submission both provides backgr',\n",
      "        'score': 14.564885139465332},\n",
      "    {   'answer': 'Health and Human Services',\n",
      "        'context': 'nd Quality (AHRQ). As part of the budget for the '\n",
      "                   'Department of Health and Human Services (HHS), the '\n",
      "                   'performance budget submission both provides backgr',\n",
      "        'score': 14.33227825164795},\n",
      "    {   'answer': 'Health and Human Services',\n",
      "        'context': 'r FY 2016 by AHRQ. As part of the budget for the '\n",
      "                   'Department of Health and Human Services (HHS), the '\n",
      "                   'performance budget submission both provides backgr',\n",
      "        'score': 11.072346687316895}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# You can configure how many candidates the reader and retriever shall return\n",
    "# The higher top_k_retriever, the better (but also the slower) your answers. \n",
    "question = \"What department is AHRQ a part of?\"\n",
    "prediction = finder.get_answers(question, top_k_retriever=10, top_k_reader=5)\n",
    "\n",
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/10/2020 18:15:35 - INFO - elasticsearch -   POST http://localhost:9200/ahrq/_search [status:200 request:0.014s]\n",
      "11/10/2020 18:15:35 - INFO - haystack.retriever.sparse -   Got 10 candidates from retriever\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<haystack.schema.Document at 0x2b3a096d3d0>,\n",
       " <haystack.schema.Document at 0x2b3a096d370>,\n",
       " <haystack.schema.Document at 0x2b3922b3fd0>,\n",
       " <haystack.schema.Document at 0x2b3f8c52f70>,\n",
       " <haystack.schema.Document at 0x2b3f8c526d0>,\n",
       " <haystack.schema.Document at 0x2b3a0843190>,\n",
       " <haystack.schema.Document at 0x2b3a0843dc0>,\n",
       " <haystack.schema.Document at 0x2b3a08434f0>,\n",
       " <haystack.schema.Document at 0x2b3a08432e0>,\n",
       " <haystack.schema.Document at 0x2b3a08432b0>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objs = finder.retriever.retrieve(question, filters=None, top_k=10, index=\"ahrq\")\n",
    "dicts = [o.to_dict() for o in objs]\n",
    "dicts[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.44 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 39.99 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.04 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 41.65 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 43.48 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 43.49 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.85 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 30.95 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 43.49 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 43.49 Batches/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What department is AHRQ a part of?',\n",
       " 'no_ans_gap': 12.131163120269775,\n",
       " 'answers': [{'answer': 'Health and Human Services',\n",
       "   'score': 14.955009460449219,\n",
       "   'probability': 0.866386080089388,\n",
       "   'context': 'nd Quality (AHRQ). As part of the budget for the Department of Health and Human Services (HHS), the performance budget submission both provides backgr',\n",
       "   'offset_start': 63,\n",
       "   'offset_end': 88,\n",
       "   'offset_start_in_doc': 821,\n",
       "   'offset_end_in_doc': 846,\n",
       "   'document_id': '929e7758-0c10-43ac-bf4f-3efd4645d700'}]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dicts[0:1]\n",
    "reader.predict(question, objs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "farm.data_handler.processor.SquadProcessor"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reader.inferencer.processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type(reader.inferencer.model.language_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.container.ModuleList'>\n",
      "<bound method QuestionAnsweringHead.aggregate_preds of QuestionAnsweringHead(\n",
      "  (feed_forward): FeedForwardBlock(\n",
      "    (feed_forward): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")>\n",
      "<class 'farm.modeling.prediction_head.QuestionAnsweringHead'>\n"
     ]
    }
   ],
   "source": [
    "a = reader.inferencer.model.prediction_heads\n",
    "print(type(a))\n",
    "if len(a) > 0:\n",
    "    print(a[0].aggregate_preds)\n",
    "print(type(a[0]))\n",
    "del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_roberta.RobertaModel"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reader.inferencer.model.language_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_convert_head_mask_to_5d',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_unimplemented',\n",
       " '_generate_beam_search',\n",
       " '_generate_no_beam_search',\n",
       " '_get_name',\n",
       " '_get_resized_embeddings',\n",
       " '_hook_rss_memory_post_forward',\n",
       " '_hook_rss_memory_pre_forward',\n",
       " '_init_weights',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_prune_heads',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_reorder_cache',\n",
       " '_replicate_for_data_parallel',\n",
       " '_resize_token_embeddings',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_tie_encoder_decoder_weights',\n",
       " '_tie_or_clone_weights',\n",
       " '_version',\n",
       " 'add_memory_hooks',\n",
       " 'add_module',\n",
       " 'adjust_logits_during_generation',\n",
       " 'apply',\n",
       " 'authorized_missing_keys',\n",
       " 'base_model',\n",
       " 'base_model_prefix',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'config',\n",
       " 'config_class',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'device',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dummy_inputs',\n",
       " 'dump_patches',\n",
       " 'embeddings',\n",
       " 'encoder',\n",
       " 'enforce_repetition_penalty_',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'from_pretrained',\n",
       " 'generate',\n",
       " 'get_extended_attention_mask',\n",
       " 'get_head_mask',\n",
       " 'get_input_embeddings',\n",
       " 'get_output_embeddings',\n",
       " 'half',\n",
       " 'init_weights',\n",
       " 'invert_attention_mask',\n",
       " 'load_state_dict',\n",
       " 'load_tf_weights',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_parameters',\n",
       " 'parameters',\n",
       " 'pooler',\n",
       " 'postprocess_next_token_scores',\n",
       " 'prepare_inputs_for_generation',\n",
       " 'prune_heads',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'reset_memory_hooks_state',\n",
       " 'resize_token_embeddings',\n",
       " 'save_pretrained',\n",
       " 'set_input_embeddings',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'tie_weights',\n",
       " 'to',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(reader.inferencer.model.language_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
