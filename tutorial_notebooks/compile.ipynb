{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, max_length = 512)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Reader():\n",
    "    def __init__(self, index, host = 'localhost', port=9200):\n",
    "        self.client = Elasticsearch(hosts=[{\"host\": host, \"port\": port}])\n",
    "        self.index = index\n",
    "        self.max_length = 0\n",
    "        model_name = \"deepset/roberta-base-squad2\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, max_length=512)\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "#         self.model.to('cuda')\n",
    "        \n",
    "    def query(self, query, fields=['text'], filters=[], top_k=10, excluded_meta_data=[]):\n",
    "        pass\n",
    "    \n",
    "        body = {\n",
    "            \"size\": top_k,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": {\n",
    "                        \"multi_match\": {\n",
    "                            \"query\": query, \n",
    "                            \"type\": \"most_fields\", \n",
    "                            \"fields\": fields\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        } \n",
    "        \n",
    "        if filters:\n",
    "            filter_clause = []\n",
    "            for key, values in filters.items():\n",
    "                filter_clause.append(\n",
    "                    {\n",
    "                        \"terms\": {key: values}\n",
    "                    }\n",
    "                )\n",
    "            body[\"query\"][\"bool\"][\"filter\"] = filter_clause\n",
    "\n",
    "        if excluded_meta_data:\n",
    "            body[\"_source\"] = {\"excludes\": excluded_meta_data}\n",
    "\n",
    "        result = self.client.search(index=self.index, body=body)[\"hits\"][\"hits\"]\n",
    "        self.contexts = [r['_source']['text'] for r in result]\n",
    "        return self.contexts\n",
    "    \n",
    "    def infer(self, query):\n",
    "        self.query(query)\n",
    "        \n",
    "        self.outputs = []\n",
    "        self.answers = []\n",
    "        for context in self.contexts:\n",
    "            inputs = self.tokenizer(query, context, return_tensors=\"pt\", add_special_tokens=True)\n",
    "            input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "            # Get longest question to test out saved model. We can pad shorter answers. Probably better than trimming longer ones\n",
    "            l = len(input_ids)\n",
    "            if l >= 512:\n",
    "                continue\n",
    "            elif l > self.max_length:\n",
    "                self.longest = (inputs['input_ids'], inputs['attention_mask'])\n",
    "                self.max_length = l\n",
    "\n",
    "            # decoded_inputs = tokenizer.decode(inputs[\"input_ids\"][0])\n",
    "            # text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "            outputs = model(**inputs)\n",
    "            self.outputs.append(outputs)\n",
    "            \n",
    "            answer = {}\n",
    "            answer_start_scores = outputs[0]\n",
    "            answer_end_scores = outputs[1]\n",
    "            answer_start = torch.argmax(answer_start_scores)  # Get the most likely beginning of answer with the argmax of the score\n",
    "            answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "            response = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "            \n",
    "            answer['score'] = max([answer_start_scores[0][answer_start].item(), answer_end_scores[0][answer_end].item()])\n",
    "            answer['answer'] = response\n",
    "            self.answers.append(answer)\n",
    "            \n",
    "#             print(f\"Question: {query}\")\n",
    "#             print(f\"Answer: {answer}\")\n",
    "            \n",
    "        self.answers.sort(key=lambda x: x['score'], reverse = True)   \n",
    "        return self.answers\n",
    "    \n",
    "    \n",
    "#     def parse(self, query, outputs=None):\n",
    "#         if not outputs:\n",
    "#             self.infer(query)\n",
    "#             outputs = self.infer(query)\n",
    "            \n",
    "#         self.answers = []\n",
    "#         for output in outputs:\n",
    "#             answer = {}\n",
    "#             answer_start_scores = output[0]\n",
    "#             answer_end_scores = output[1]\n",
    "#             answer_start = torch.argmax(\n",
    "#                 answer_start_scores\n",
    "#             )  # Get the most likely beginning of answer with the argmax of the score\n",
    "#             answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "#             response = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "#             answer['score'] = max([answer_start, answer_end])\n",
    "#             answer['answer'] = response\n",
    "            \n",
    "#             print(f\"Question: {query}\")\n",
    "#             print(f\"Answer: {answer}\")\n",
    "#         self.answers.sort(key=lambda x: x['score'], reverse = True)\n",
    "        \n",
    "reader = Reader('ahrq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 4.527154922485352,\n",
       "  'answer': ' Agency for Healthcare Research and Quality'},\n",
       " {'score': 4.409052848815918, 'answer': ''},\n",
       " {'score': 4.211277961730957, 'answer': ''},\n",
       " {'score': 2.202496290206909, 'answer': '<s>'},\n",
       " {'score': 1.9319677352905273,\n",
       "  'answer': ' a consolidated set of national standardized databases of reliable social factors that will build on existing databases developed by Federal agencies'},\n",
       " {'score': 1.4219703674316406,\n",
       "  'answer': '<s>What is ahrq?</s></s>Dr. Embi noted the advancing capabilities of technologies, which can lead to better care management. Multidirectional communication is now possible. AHRQ could enable and encourage solutions that allow such communication. As such, AHRQ could advance innovation in patient-centered care'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is ahrq?\"\n",
    "reader.infer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Agency for Healthcare Research and Quality'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.answers[0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
